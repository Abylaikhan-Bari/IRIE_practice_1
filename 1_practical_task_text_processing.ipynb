{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Практическое задание 1: Введение в NLTK, spaCy и обработку текста\n",
    "**Курс:** Information Retrieval & Information Extraction\n",
    "**Автор:** Абылайхан Бари\n",
    "\n",
    "Этот отчет содержит реализацию основных операций обработки текста:\n",
    "✅ Токенизация (NLTK, spaCy)\n",
    "✅ Удаление стоп-слов (NLTK, spaCy)\n",
    "✅ Сравнение результатов\n",
    "✅ Анализ работы с русским текстом\n"
   ],
   "id": "cd9fdad5c79e3c44"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T11:10:24.207110Z",
     "start_time": "2025-02-13T11:03:48.104277Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!pip install nltk spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download ru_core_news_sm\n"
   ],
   "id": "1be84c1c8ce43ef7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in ./.venv/lib/python3.9/site-packages (3.9.1)\r\n",
      "Requirement already satisfied: spacy in ./.venv/lib/python3.9/site-packages (3.8.3)\r\n",
      "Requirement already satisfied: click in ./.venv/lib/python3.9/site-packages (from nltk) (8.1.8)\r\n",
      "Requirement already satisfied: joblib in ./.venv/lib/python3.9/site-packages (from nltk) (1.4.2)\r\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./.venv/lib/python3.9/site-packages (from nltk) (2024.11.6)\r\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.9/site-packages (from nltk) (4.67.1)\r\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in ./.venv/lib/python3.9/site-packages (from spacy) (3.0.12)\r\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in ./.venv/lib/python3.9/site-packages (from spacy) (1.0.5)\r\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./.venv/lib/python3.9/site-packages (from spacy) (1.0.12)\r\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./.venv/lib/python3.9/site-packages (from spacy) (2.0.11)\r\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./.venv/lib/python3.9/site-packages (from spacy) (3.0.9)\r\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.0 in ./.venv/lib/python3.9/site-packages (from spacy) (8.3.4)\r\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in ./.venv/lib/python3.9/site-packages (from spacy) (1.1.3)\r\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in ./.venv/lib/python3.9/site-packages (from spacy) (2.5.1)\r\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in ./.venv/lib/python3.9/site-packages (from spacy) (2.0.10)\r\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in ./.venv/lib/python3.9/site-packages (from spacy) (0.4.1)\r\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in ./.venv/lib/python3.9/site-packages (from spacy) (0.15.1)\r\n",
      "Requirement already satisfied: numpy>=1.19.0 in ./.venv/lib/python3.9/site-packages (from spacy) (2.0.2)\r\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./.venv/lib/python3.9/site-packages (from spacy) (2.32.3)\r\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in ./.venv/lib/python3.9/site-packages (from spacy) (2.10.6)\r\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.9/site-packages (from spacy) (3.1.5)\r\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.9/site-packages (from spacy) (68.2.0)\r\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.9/site-packages (from spacy) (24.2)\r\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in ./.venv/lib/python3.9/site-packages (from spacy) (3.5.0)\r\n",
      "Requirement already satisfied: language-data>=1.2 in ./.venv/lib/python3.9/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in ./.venv/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\r\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in ./.venv/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\r\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in ./.venv/lib/python3.9/site-packages (from thinc<8.4.0,>=8.3.0->spacy) (1.2.0)\r\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in ./.venv/lib/python3.9/site-packages (from thinc<8.4.0,>=8.3.0->spacy) (0.1.5)\r\n",
      "Requirement already satisfied: shellingham>=1.3.0 in ./.venv/lib/python3.9/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\r\n",
      "Requirement already satisfied: rich>=10.11.0 in ./.venv/lib/python3.9/site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\r\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in ./.venv/lib/python3.9/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\r\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in ./.venv/lib/python3.9/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.9/site-packages (from jinja2->spacy) (3.0.2)\r\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in ./.venv/lib/python3.9/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.9/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.9/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\r\n",
      "Requirement already satisfied: wrapt in ./.venv/lib/python3.9/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\r\n",
      "/Users/aikei/PycharmProjects/IRIE_practice_1/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\r\n",
      "  warnings.warn(\r\n",
      "Collecting en-core-web-sm==3.8.0\r\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\r\n",
      "\u001B[2K     \u001B[91m━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.3/12.8 MB\u001B[0m \u001B[31m37.6 kB/s\u001B[0m eta \u001B[36m0:05:07\u001B[0m\r\n",
      "\u001B[?25h\u001B[31mERROR: Exception:\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/Users/aikei/PycharmProjects/IRIE_practice_1/.venv/lib/python3.9/site-packages/pip/_vendor/urllib3/response.py\", line 438, in _error_catcher\r\n",
      "    yield\r\n",
      "  File \"/Users/aikei/PycharmProjects/IRIE_practice_1/.venv/lib/python3.9/site-packages/pip/_vendor/urllib3/response.py\", line 561, in read\r\n",
      "    data = self._fp_read(amt) if not fp_closed else b\"\"\r\n",
      "  File \"/Users/aikei/PycharmProjects/IRIE_practice_1/.venv/lib/python3.9/site-packages/pip/_vendor/urllib3/response.py\", line 527, in _fp_read\r\n",
      "    return self._fp.read(amt) if amt is not None else self._fp.read()\r\n",
      "  File \"/Users/aikei/PycharmProjects/IRIE_practice_1/.venv/lib/python3.9/site-packages/pip/_vendor/cachecontrol/filewrapper.py\", line 98, in read\r\n",
      "    data: bytes = self.__fp.read(amt)\r\n",
      "  File \"/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/http/client.py\", line 459, in read\r\n",
      "    n = self.readinto(b)\r\n",
      "  File \"/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/http/client.py\", line 503, in readinto\r\n",
      "    n = self.fp.readinto(b)\r\n",
      "  File \"/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/socket.py\", line 704, in readinto\r\n",
      "    return self._sock.recv_into(b)\r\n",
      "  File \"/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/ssl.py\", line 1241, in recv_into\r\n",
      "    return self.read(nbytes, buffer)\r\n",
      "  File \"/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/ssl.py\", line 1099, in read\r\n",
      "    return self._sslobj.read(len, buffer)\r\n",
      "socket.timeout: The read operation timed out\r\n",
      "\r\n",
      "During handling of the above exception, another exception occurred:\r\n",
      "\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/Users/aikei/PycharmProjects/IRIE_practice_1/.venv/lib/python3.9/site-packages/pip/_internal/cli/base_command.py\", line 106, in _run_wrapper\r\n",
      "    status = _inner_run()\r\n",
      "  File \"/Users/aikei/PycharmProjects/IRIE_practice_1/.venv/lib/python3.9/site-packages/pip/_internal/cli/base_command.py\", line 97, in _inner_run\r\n",
      "    return self.run(options, args)\r\n",
      "  File \"/Users/aikei/PycharmProjects/IRIE_practice_1/.venv/lib/python3.9/site-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\r\n",
      "    return func(self, options, args)\r\n",
      "  File \"/Users/aikei/PycharmProjects/IRIE_practice_1/.venv/lib/python3.9/site-packages/pip/_internal/commands/install.py\", line 386, in run\r\n",
      "    requirement_set = resolver.resolve(\r\n",
      "  File \"/Users/aikei/PycharmProjects/IRIE_practice_1/.venv/lib/python3.9/site-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 76, in resolve\r\n",
      "    collected = self.factory.collect_root_requirements(root_reqs)\r\n",
      "  File \"/Users/aikei/PycharmProjects/IRIE_practice_1/.venv/lib/python3.9/site-packages/pip/_internal/resolution/resolvelib/factory.py\", line 545, in collect_root_requirements\r\n",
      "    reqs = list(\r\n",
      "  File \"/Users/aikei/PycharmProjects/IRIE_practice_1/.venv/lib/python3.9/site-packages/pip/_internal/resolution/resolvelib/factory.py\", line 501, in _make_requirements_from_install_req\r\n",
      "    cand = self._make_base_candidate_from_link(\r\n",
      "  File \"/Users/aikei/PycharmProjects/IRIE_practice_1/.venv/lib/python3.9/site-packages/pip/_internal/resolution/resolvelib/factory.py\", line 233, in _make_base_candidate_from_link\r\n",
      "    self._link_candidate_cache[link] = LinkCandidate(\r\n",
      "  File \"/Users/aikei/PycharmProjects/IRIE_practice_1/.venv/lib/python3.9/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 304, in __init__\r\n",
      "    super().__init__(\r\n",
      "  File \"/Users/aikei/PycharmProjects/IRIE_practice_1/.venv/lib/python3.9/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 159, in __init__\r\n",
      "    self.dist = self._prepare()\r\n",
      "  File \"/Users/aikei/PycharmProjects/IRIE_practice_1/.venv/lib/python3.9/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 236, in _prepare\r\n",
      "    dist = self._prepare_distribution()\r\n",
      "  File \"/Users/aikei/PycharmProjects/IRIE_practice_1/.venv/lib/python3.9/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 315, in _prepare_distribution\r\n",
      "    return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True)\r\n",
      "  File \"/Users/aikei/PycharmProjects/IRIE_practice_1/.venv/lib/python3.9/site-packages/pip/_internal/operations/prepare.py\", line 527, in prepare_linked_requirement\r\n",
      "    return self._prepare_linked_requirement(req, parallel_builds)\r\n",
      "  File \"/Users/aikei/PycharmProjects/IRIE_practice_1/.venv/lib/python3.9/site-packages/pip/_internal/operations/prepare.py\", line 598, in _prepare_linked_requirement\r\n",
      "    local_file = unpack_url(\r\n",
      "  File \"/Users/aikei/PycharmProjects/IRIE_practice_1/.venv/lib/python3.9/site-packages/pip/_internal/operations/prepare.py\", line 170, in unpack_url\r\n",
      "    file = get_http_url(\r\n",
      "  File \"/Users/aikei/PycharmProjects/IRIE_practice_1/.venv/lib/python3.9/site-packages/pip/_internal/operations/prepare.py\", line 111, in get_http_url\r\n",
      "    from_path, content_type = download(link, temp_dir.path)\r\n",
      "  File \"/Users/aikei/PycharmProjects/IRIE_practice_1/.venv/lib/python3.9/site-packages/pip/_internal/network/download.py\", line 148, in __call__\r\n",
      "    for chunk in chunks:\r\n",
      "  File \"/Users/aikei/PycharmProjects/IRIE_practice_1/.venv/lib/python3.9/site-packages/pip/_internal/cli/progress_bars.py\", line 55, in _rich_progress_bar\r\n",
      "    for chunk in iterable:\r\n",
      "  File \"/Users/aikei/PycharmProjects/IRIE_practice_1/.venv/lib/python3.9/site-packages/pip/_internal/network/utils.py\", line 65, in response_chunks\r\n",
      "    for chunk in response.raw.stream(\r\n",
      "  File \"/Users/aikei/PycharmProjects/IRIE_practice_1/.venv/lib/python3.9/site-packages/pip/_vendor/urllib3/response.py\", line 622, in stream\r\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\r\n",
      "  File \"/Users/aikei/PycharmProjects/IRIE_practice_1/.venv/lib/python3.9/site-packages/pip/_vendor/urllib3/response.py\", line 587, in read\r\n",
      "    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)\r\n",
      "  File \"/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/contextlib.py\", line 135, in __exit__\r\n",
      "    self.gen.throw(type, value, traceback)\r\n",
      "  File \"/Users/aikei/PycharmProjects/IRIE_practice_1/.venv/lib/python3.9/site-packages/pip/_vendor/urllib3/response.py\", line 443, in _error_catcher\r\n",
      "    raise ReadTimeoutError(self._pool, None, \"Read timed out.\")\r\n",
      "pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='objects.githubusercontent.com', port=443): Read timed out.\u001B[0m\u001B[31m\r\n",
      "\u001B[0m/Users/aikei/PycharmProjects/IRIE_practice_1/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\r\n",
      "  warnings.warn(\r\n",
      "Collecting ru-core-news-sm==3.8.0\r\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/ru_core_news_sm-3.8.0/ru_core_news_sm-3.8.0-py3-none-any.whl (15.3 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m15.3/15.3 MB\u001B[0m \u001B[31m42.8 kB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:02\u001B[0m00:13\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: pymorphy3>=1.0.0 in ./.venv/lib/python3.9/site-packages (from ru-core-news-sm==3.8.0) (2.0.2)\r\n",
      "Requirement already satisfied: dawg-python>=0.7.1 in ./.venv/lib/python3.9/site-packages (from pymorphy3>=1.0.0->ru-core-news-sm==3.8.0) (0.7.2)\r\n",
      "Requirement already satisfied: pymorphy3-dicts-ru in ./.venv/lib/python3.9/site-packages (from pymorphy3>=1.0.0->ru-core-news-sm==3.8.0) (2.4.417150.4580142)\r\n",
      "\u001B[38;5;2m✔ Download and installation successful\u001B[0m\r\n",
      "You can now load the package via spacy.load('ru_core_news_sm')\r\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T11:25:07.042691Z",
     "start_time": "2025-02-13T11:25:06.382146Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Загрузка необходимых ресурсов NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Загрузка моделей spaCy\n",
    "nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "nlp_ru = spacy.load(\"ru_core_news_sm\")\n"
   ],
   "id": "af8a43675c2ebbf9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/aikei/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/aikei/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T11:25:13.866929Z",
     "start_time": "2025-02-13T11:25:13.863630Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text_en = \"\"\"\n",
    "Natural Language Processing (NLP) is an exciting field of artificial intelligence.\n",
    "It enables computers to understand, interpret, and respond to human language.\n",
    "Many applications, such as chatbots, translation services, and sentiment analysis,\n",
    "rely on NLP technology.\n",
    "Furthermore, machine learning and deep learning techniques have significantly improved NLP capabilities.\n",
    "\"\"\"\n",
    "\n",
    "text_ru = \"\"\"\n",
    "Обработка естественного языка — это увлекательная область искусственного интеллекта.\n",
    "Она позволяет компьютерам понимать, интерпретировать и отвечать на человеческий язык.\n",
    "Многие приложения, такие как чат-боты, переводчики и анализ настроений, используют технологии NLP.\n",
    "Кроме того, машинное обучение и глубокие нейросети значительно улучшили возможности обработки текста.\n",
    "\"\"\"\n",
    "\n",
    "print(\"English Text:\", text_en)\n",
    "print(\"Russian Text:\", text_ru)\n"
   ],
   "id": "47a803da1fa34bdf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Text: \n",
      "Natural Language Processing (NLP) is an exciting field of artificial intelligence.\n",
      "It enables computers to understand, interpret, and respond to human language.\n",
      "Many applications, such as chatbots, translation services, and sentiment analysis,\n",
      "rely on NLP technology.\n",
      "Furthermore, machine learning and deep learning techniques have significantly improved NLP capabilities.\n",
      "\n",
      "Russian Text: \n",
      "Обработка естественного языка — это увлекательная область искусственного интеллекта.\n",
      "Она позволяет компьютерам понимать, интерпретировать и отвечать на человеческий язык.\n",
      "Многие приложения, такие как чат-боты, переводчики и анализ настроений, используют технологии NLP.\n",
      "Кроме того, машинное обучение и глубокие нейросети значительно улучшили возможности обработки текста.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T11:25:20.818072Z",
     "start_time": "2025-02-13T11:25:20.783780Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Task 1: Tokenization ---\n",
    "# NLTK Tokenization\n",
    "tokens_nltk_en = word_tokenize(text_en)\n",
    "tokens_nltk_ru = word_tokenize(text_ru, language=\"russian\")\n",
    "\n",
    "# spaCy Tokenization\n",
    "doc_en = nlp_en(text_en)\n",
    "tokens_spacy_en = [token.text for token in doc_en]\n",
    "\n",
    "doc_ru = nlp_ru(text_ru)\n",
    "tokens_spacy_ru = [token.text for token in doc_ru]\n",
    "\n",
    "print(\"🔹 English Tokenization (NLTK):\", tokens_nltk_en)\n",
    "print(\"🔹 English Tokenization (spaCy):\", tokens_spacy_en)\n",
    "print(\"🔹 Russian Tokenization (NLTK):\", tokens_nltk_ru)\n",
    "print(\"🔹 Russian Tokenization (spaCy):\", tokens_spacy_ru)\n"
   ],
   "id": "89ee33718ca0fb88",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 English Tokenization (NLTK): ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'an', 'exciting', 'field', 'of', 'artificial', 'intelligence', '.', 'It', 'enables', 'computers', 'to', 'understand', ',', 'interpret', ',', 'and', 'respond', 'to', 'human', 'language', '.', 'Many', 'applications', ',', 'such', 'as', 'chatbots', ',', 'translation', 'services', ',', 'and', 'sentiment', 'analysis', ',', 'rely', 'on', 'NLP', 'technology', '.', 'Furthermore', ',', 'machine', 'learning', 'and', 'deep', 'learning', 'techniques', 'have', 'significantly', 'improved', 'NLP', 'capabilities', '.']\n",
      "🔹 English Tokenization (spaCy): ['\\n', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'an', 'exciting', 'field', 'of', 'artificial', 'intelligence', '.', '\\n', 'It', 'enables', 'computers', 'to', 'understand', ',', 'interpret', ',', 'and', 'respond', 'to', 'human', 'language', '.', '\\n', 'Many', 'applications', ',', 'such', 'as', 'chatbots', ',', 'translation', 'services', ',', 'and', 'sentiment', 'analysis', ',', '\\n', 'rely', 'on', 'NLP', 'technology', '.', '\\n', 'Furthermore', ',', 'machine', 'learning', 'and', 'deep', 'learning', 'techniques', 'have', 'significantly', 'improved', 'NLP', 'capabilities', '.', '\\n']\n",
      "🔹 Russian Tokenization (NLTK): ['Обработка', 'естественного', 'языка', '—', 'это', 'увлекательная', 'область', 'искусственного', 'интеллекта', '.', 'Она', 'позволяет', 'компьютерам', 'понимать', ',', 'интерпретировать', 'и', 'отвечать', 'на', 'человеческий', 'язык', '.', 'Многие', 'приложения', ',', 'такие', 'как', 'чат-боты', ',', 'переводчики', 'и', 'анализ', 'настроений', ',', 'используют', 'технологии', 'NLP', '.', 'Кроме', 'того', ',', 'машинное', 'обучение', 'и', 'глубокие', 'нейросети', 'значительно', 'улучшили', 'возможности', 'обработки', 'текста', '.']\n",
      "🔹 Russian Tokenization (spaCy): ['\\n', 'Обработка', 'естественного', 'языка', '—', 'это', 'увлекательная', 'область', 'искусственного', 'интеллекта', '.', '\\n', 'Она', 'позволяет', 'компьютерам', 'понимать', ',', 'интерпретировать', 'и', 'отвечать', 'на', 'человеческий', 'язык', '.', '\\n', 'Многие', 'приложения', ',', 'такие', 'как', 'чат', '-', 'боты', ',', 'переводчики', 'и', 'анализ', 'настроений', ',', 'используют', 'технологии', 'NLP', '.', '\\n', 'Кроме', 'того', ',', 'машинное', 'обучение', 'и', 'глубокие', 'нейросети', 'значительно', 'улучшили', 'возможности', 'обработки', 'текста', '.', '\\n']\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T11:25:25.974959Z",
     "start_time": "2025-02-13T11:25:25.969166Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Task 2: Stop-word Removal ---\n",
    "# NLTK Stop-word removal\n",
    "stop_words_en = set(stopwords.words('english'))\n",
    "filtered_words_nltk_en = [word for word in tokens_nltk_en if word.lower() not in stop_words_en]\n",
    "\n",
    "stop_words_ru = set(stopwords.words('russian'))\n",
    "filtered_words_nltk_ru = [word for word in tokens_nltk_ru if word.lower() not in stop_words_ru]\n",
    "\n",
    "# spaCy Stop-word removal\n",
    "filtered_words_spacy_en = [token.text for token in doc_en if not token.is_stop]\n",
    "filtered_words_spacy_ru = [token.text for token in doc_ru if not token.is_stop]\n",
    "\n",
    "print(\"🔹 English Stop-word Removal (NLTK):\", filtered_words_nltk_en)\n",
    "print(\"🔹 English Stop-word Removal (spaCy):\", filtered_words_spacy_en)\n",
    "print(\"🔹 Russian Stop-word Removal (NLTK):\", filtered_words_nltk_ru)\n",
    "print(\"🔹 Russian Stop-word Removal (spaCy):\", filtered_words_spacy_ru)\n"
   ],
   "id": "11a50381eff8db35",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 English Stop-word Removal (NLTK): ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'exciting', 'field', 'artificial', 'intelligence', '.', 'enables', 'computers', 'understand', ',', 'interpret', ',', 'respond', 'human', 'language', '.', 'Many', 'applications', ',', 'chatbots', ',', 'translation', 'services', ',', 'sentiment', 'analysis', ',', 'rely', 'NLP', 'technology', '.', 'Furthermore', ',', 'machine', 'learning', 'deep', 'learning', 'techniques', 'significantly', 'improved', 'NLP', 'capabilities', '.']\n",
      "🔹 English Stop-word Removal (spaCy): ['\\n', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'exciting', 'field', 'artificial', 'intelligence', '.', '\\n', 'enables', 'computers', 'understand', ',', 'interpret', ',', 'respond', 'human', 'language', '.', '\\n', 'applications', ',', 'chatbots', ',', 'translation', 'services', ',', 'sentiment', 'analysis', ',', '\\n', 'rely', 'NLP', 'technology', '.', '\\n', 'Furthermore', ',', 'machine', 'learning', 'deep', 'learning', 'techniques', 'significantly', 'improved', 'NLP', 'capabilities', '.', '\\n']\n",
      "🔹 Russian Stop-word Removal (NLTK): ['Обработка', 'естественного', 'языка', '—', 'это', 'увлекательная', 'область', 'искусственного', 'интеллекта', '.', 'позволяет', 'компьютерам', 'понимать', ',', 'интерпретировать', 'отвечать', 'человеческий', 'язык', '.', 'Многие', 'приложения', ',', 'такие', 'чат-боты', ',', 'переводчики', 'анализ', 'настроений', ',', 'используют', 'технологии', 'NLP', '.', 'Кроме', ',', 'машинное', 'обучение', 'глубокие', 'нейросети', 'значительно', 'улучшили', 'возможности', 'обработки', 'текста', '.']\n",
      "🔹 Russian Stop-word Removal (spaCy): ['\\n', 'Обработка', 'естественного', 'языка', '—', 'увлекательная', 'область', 'искусственного', 'интеллекта', '.', '\\n', 'позволяет', 'компьютерам', 'понимать', ',', 'интерпретировать', 'отвечать', 'человеческий', 'язык', '.', '\\n', 'приложения', ',', 'чат', '-', 'боты', ',', 'переводчики', 'анализ', 'настроений', ',', 'используют', 'технологии', 'NLP', '.', '\\n', ',', 'машинное', 'обучение', 'глубокие', 'нейросети', 'значительно', 'улучшили', 'возможности', 'обработки', 'текста', '.', '\\n']\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T11:25:31.147613Z",
     "start_time": "2025-02-13T11:25:31.143665Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"🔍 Words removed by NLTK but kept by spaCy (English):\", set(filtered_words_nltk_en) - set(filtered_words_spacy_en))\n",
    "print(\"🔍 Words removed by spaCy but kept by NLTK (English):\", set(filtered_words_spacy_en) - set(filtered_words_nltk_en))\n",
    "\n",
    "print(\"🔍 Words removed by NLTK but kept by spaCy (Russian):\", set(filtered_words_nltk_ru) - set(filtered_words_spacy_ru))\n",
    "print(\"🔍 Words removed by spaCy but kept by NLTK (Russian):\", set(filtered_words_spacy_ru) - set(filtered_words_nltk_ru))\n"
   ],
   "id": "73224041273b1ff8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Words removed by NLTK but kept by spaCy (English): {'Many'}\n",
      "🔍 Words removed by spaCy but kept by NLTK (English): {'\\n'}\n",
      "🔍 Words removed by NLTK but kept by spaCy (Russian): {'чат-боты', 'Кроме', 'Многие', 'такие', 'это'}\n",
      "🔍 Words removed by spaCy but kept by NLTK (Russian): {'\\n', 'чат', 'боты', '-'}\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Выводы\n",
    "1. **Токенизация**:\n",
    "   - NLTK разбивает текст просто на слова и знаки препинания.\n",
    "   - spaCy использует более сложные алгоритмы и корректно обрабатывает знаки препинания.\n",
    "\n",
    "2. **Удаление стоп-слов**:\n",
    "   - NLTK использует фиксированный список стоп-слов.\n",
    "   - spaCy использует динамический список, основанный на моделях.\n",
    "\n",
    "3. **Различия между NLTK и spaCy**:\n",
    "   - spaCy чаще оставляет важные слова, даже если они есть в списке стоп-слов NLTK.\n",
    "   - В русском языке spaCy работает точнее, так как у NLTK могут отсутствовать некоторые слова.\n",
    "\n",
    "4. **Вывод по русскому языку**:\n",
    "   - NLTK хуже справляется с токенизацией русского текста.\n",
    "   - spaCy лучше распознает границы слов и знаки препинания.\n",
    "\n",
    "**💡 Рекомендация:** Для большинства задач spaCy более удобен, особенно для работы с русским текстом.\n"
   ],
   "id": "78cdab209104bf1b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ad5762ffbd73be75"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
